{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaviRamos/NLP/blob/master/Spacy3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwnC5w1ffoAA",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization\n",
        "Uma boa forma de começar a analisar o texto é o separando em unidades menores. O nome desse processo é tokenization, que basicamente significa separar o texto em palavras.\n",
        "\n",
        "No spaCy você utiliza models, que são tipo os ‘cérebros treinados’ de cada língua. A biblioteca ainda não tem um model para Português :(, só para Inglês e Alemão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlrJc-LOfYuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "05be0447-ee41-4727-c87f-87121de1c5c7"
      },
      "source": [
        "pip install -U Spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: Spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.0.7)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from Spacy) (7.0.4)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.2.4)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->Spacy) (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfxLpsqIfhwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5da7ccd1-c12c-4b69-960d-8634f0dd5814"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaOkXHX8ggSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iA9mJgKfxhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = \"Seven years after the death of his wife, Mill was invited to contest Westminster. His feeling on the conduct of elections made him refuse to take any personal action in the matter, and he gave the frankest expression to his political views, but nevertheless he was elected by a large majority. He was not a conventional success in the House; as a speaker he lacked magnetism. But his influence was widely felt.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b12SogEJgD-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsedData = nlp(raw_text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX0oygCggmBV",
        "colab_type": "text"
      },
      "source": [
        "Ao passar o texto para o model ele já o separa em tokens e computa várias outras propriedades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R85yrfU-giMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bfc430a-d93a-4701-9475-c710b7d2fb3b"
      },
      "source": [
        "word = parsedData[0]\n",
        "print(word.text, word.lower_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seven seven\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VGl0Y9UgtHV",
        "colab_type": "text"
      },
      "source": [
        "# Part-of-speech tagging\n",
        "Lembra na escola quando a gente tinha que identificar quem era o substantivo, o verbo e os adjetivos nas frases? Em NLP esse processo se chama part-of-speech tagging. É a boa e velha análise gramatical.\n",
        "\n",
        "O model já fez essa análise, que pode ser acessada pela propriedade.pos_ de cada token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_9WPIzogok_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5fde63fc-ff56-43ed-eb8b-b503b4cd3ecd"
      },
      "source": [
        "for i,word in enumerate(parsedData):\n",
        "    print(word.text, word.pos_)\n",
        "    if i > 5:\n",
        "        break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seven NUM\n",
            "years NOUN\n",
            "after ADP\n",
            "the DET\n",
            "death NOUN\n",
            "of ADP\n",
            "his DET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NC2MskQg-u4",
        "colab_type": "text"
      },
      "source": [
        "[Aqui](https://spacy.io/usage/linguistic-features#pos-tagging) a gente pode ver a lista com o significado de cada uma dessas tags.\n",
        "\n",
        "Existe também uma tag mais elaborada, que pode ser acessada pela propriedade .tag_ Essa tag contém informações sobre a estrutura morfológica da palavra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLNw2hTUgw0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4b841284-f3c3-428a-9efe-0f492290fb83"
      },
      "source": [
        "word = parsedData[10] #a palavra 'was'\n",
        "print(\"original:\",word.text)\n",
        "print(\"POS tag:\",word.pos_)\n",
        "print(\"fine grainned POS tag:\", word.tag_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: was\n",
            "POS tag: VERB\n",
            "fine grainned POS tag: VBD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axBpoosrhNfg",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition (NER)\n",
        "Esse processo busca identificar categorias como nomes de pessoas, organizações, locais, porcentagens, valores monetários, e por ai vai. Essas categorias podem ser pré-definidas por nós, então dependendo do texto podemos criar nossas próprias entidades.\n",
        "\n",
        "Pra variar, o model também já computou isso pra a gente na propriedade .ent_type_. Vamos ver quantas entidades nosso texto tem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVuw35fehHZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f86c4e43-4be2-4adf-fbcc-cfa69a488223"
      },
      "source": [
        "for word in parsedData:\n",
        "    if word.ent_type_:\n",
        "        print(word.text, word.ent_type_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seven DATE\n",
            "years DATE\n",
            "Mill PERSON\n",
            "Westminster PERSON\n",
            "House ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4SlKEuuhVnk",
        "colab_type": "text"
      },
      "source": [
        "# Syntactic Parsing\n",
        "Syntactic parsing é o processo de representar as dependências do texto através das relações entre os tokens. Por exemplo: um artigo está ligado a um substantivo, um advérbio modifica um verbo, e por ai vai.\n",
        "\n",
        "O[ Explosion AI ](https://explosion.ai/)criou uma ferramenta linda para visualizar essas dependências, o [Dependency Visualizer](https://explosion.ai/demos/displacy).\n",
        "\n",
        "O atributo para ter acesso à dependência sintática de cada token é o .dep_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjdiDWphQZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f18a3c4-f01c-4436-a6fd-1622e41b623f"
      },
      "source": [
        "for word in parsedData:\n",
        "    print(word.text, word.dep_)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seven nummod\n",
            "years npadvmod\n",
            "after prep\n",
            "the det\n",
            "death pobj\n",
            "of prep\n",
            "his poss\n",
            "wife pobj\n",
            ", punct\n",
            "Mill nsubjpass\n",
            "was auxpass\n",
            "invited ROOT\n",
            "to aux\n",
            "contest xcomp\n",
            "Westminster dobj\n",
            ". punct\n",
            "His poss\n",
            "feeling nsubj\n",
            "on prep\n",
            "the det\n",
            "conduct pobj\n",
            "of prep\n",
            "elections pobj\n",
            "made ROOT\n",
            "him nsubj\n",
            "refuse ccomp\n",
            "to aux\n",
            "take xcomp\n",
            "any det\n",
            "personal amod\n",
            "action dobj\n",
            "in prep\n",
            "the det\n",
            "matter pobj\n",
            ", punct\n",
            "and cc\n",
            "he nsubj\n",
            "gave conj\n",
            "the det\n",
            "frankest amod\n",
            "expression dobj\n",
            "to dative\n",
            "his poss\n",
            "political amod\n",
            "views pobj\n",
            ", punct\n",
            "but cc\n",
            "nevertheless advmod\n",
            "he nsubjpass\n",
            "was auxpass\n",
            "elected conj\n",
            "by agent\n",
            "a det\n",
            "large amod\n",
            "majority pobj\n",
            ". punct\n",
            "He nsubj\n",
            "was ccomp\n",
            "not neg\n",
            "a det\n",
            "conventional amod\n",
            "success attr\n",
            "in prep\n",
            "the det\n",
            "House pobj\n",
            "; punct\n",
            "as prep\n",
            "a det\n",
            "speaker pobj\n",
            "he nsubj\n",
            "lacked ROOT\n",
            "magnetism dobj\n",
            ". punct\n",
            "But cc\n",
            "his poss\n",
            "influence nsubjpass\n",
            "was auxpass\n",
            "widely advmod\n",
            "felt ROOT\n",
            ". punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDP-Zwg6h0O4",
        "colab_type": "text"
      },
      "source": [
        "Existem várias formas de navegar pela árvore sintática. [Aqui na documentação](https://spacy.io/usage/linguistic-features#dependency-parse) eles explicam tudo que pode ser feito. É possível encontrar um verbo e o sujeito dele, por exemplo.\n",
        "\n",
        "[Aqui](https://emorynlp.github.io/nlp4j/components/dependency-parsing.html) tem uma lista com o significado de cada tag da análise sintática e nesse [site aqui](https://universaldependencies.org/en/dep/) eles explicam mais o significado de cada termo.\n",
        "\n",
        "Agora vamos ver funcionalidades do spaCy que nos ajudam a trabalhar com as métricas que vimos até aqui.\n",
        "\n",
        "# Rule-based matching\n",
        "Esse rule-based matching permite a criação de regras utilizando os atributos e flags dos tokens. Isso pode ser bem interessante nos casos onde a gente tá trabalhando em um corpus sobre um domínio específico.\n",
        "\n",
        "Por exemplo, eu quero encontrar todos os sujeitos nominais do texto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLo3fd1-hwyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.attrs import DEP\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add_pattern(\"SujeitoNominal\", [ {DEP:'nsubj'}])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnl_q_VliYa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(raw_text)\n",
        "matches = matcher(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doyjsibjikot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ent_id, label, start, end in matcher(doc):\n",
        "    print(doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWWEPNNrinY7",
        "colab_type": "text"
      },
      "source": [
        "# Ainda é possível:\n",
        "\n",
        "[Criar uma regra nova para a extração dos tokens](https://spacy.io/usage/linguistic-features#tokenization)\n",
        "\n",
        "[Criar uma classe Tokenizer do zero](https://spacy.io/usage/linguistic-features#tokenization)\n",
        "\n",
        "[Usar um tokenizer arbitrário em um model](https://spacy.io/usage/linguistic-features#tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIfyqJIai_Ys",
        "colab_type": "text"
      },
      "source": [
        "# Word vectors\n",
        "Para computar as similaridades entre palavras uma técnica comum é representá-las através de vetores. A forma mais famosa de treinar esses vetores é com a família de algoritmos do [word2vec](https://en.wikipedia.org/wiki/Word2vec).\n",
        "\n",
        "Com o spaCy é fácil trabalhar com vetores porque as classes Lexeme, Token, Span e Doc têm uma propriedade .vector . Vamos testar isso vendo as similaridades entre um cachorro, um gato e um cavalo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4aRJJLEi6O_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "1d0ecc39-0819-442b-c342-80aef00d6f7e"
      },
      "source": [
        "my,dog,and_,cat,and__,horse = nlp(u'my dog and cat and horse')\n",
        "print(cat.similarity(dog))\n",
        "print(cat.similarity(horse))\n",
        "print(dog.similarity(horse))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.61500615\n",
            "0.67795044\n",
            "0.5448442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO4MjUUEjHpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}