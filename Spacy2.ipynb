{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaviRamos/NLP/blob/master/Spacy2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGWSNEQobLFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "53936a7c-e293-4963-b7fc-c34bad24545a"
      },
      "source": [
        "pip install -U Spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: Spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.2.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from Spacy) (7.0.4)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from Spacy) (0.0.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from Spacy) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->Spacy) (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_kw9i71bgUc",
        "colab_type": "text"
      },
      "source": [
        "Usando modelos pré-prontos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX5jXUvJbO-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "42b0bb0a-a431-493a-d7c9-da2be0a18cb9"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"pt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rk_epPCbQqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1f008bc5-8bbc-4675-d4a7-e135b200cc52"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0gko2nnbVjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(u'Você encontrou o livro que eu te falei, Carla?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn-m9nwObmXk",
        "colab_type": "text"
      },
      "source": [
        "Obs: você deve declarar a string como unicode para que ele funcione corretamente.\n",
        "\n",
        "# Um pouco sobre docs e tokens…\n",
        "Um Doc, objeto como aquele que acabamos de criar, é uma sequência de objetos do tipo Token e possui diversas informações sobre o texto que ele contém. Por dividir a frase em tokens, esse documento é uma estrutura iterável e portanto, deve ser acessada como tal. Já um Token é uma parte da estrutura e pode ser uma frase, palavra, uma pontuação, um espaço em branco, etc. No nosso caso, como iremos avaliar uma frase, os tokens serão constituídos de palavras e pontuações.\n",
        "\n",
        "Primeiro, vamos analisar a frase da maneira mais simples: dividindo-a com o método split de qualquer string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9FlMbdDbnVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20f4b44f-ca0e-41a0-8320-1b34ef5e8b85"
      },
      "source": [
        "doc.text.split()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'te', 'falei,', 'Carla?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV0fMRELb-Yq",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que apesar de ser coerente a divisão por espaços, o verbo falei e a vírgula estão dentro de um mesmo token, assim como Carla e a interrogação. O nlp consegue entender a diferença entre eles e, portanto, quando usamos os tokens dentro da estrutura do documento, temos uma divisão mais coerente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMJav11db-9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a8f8ee9-3e75-4fdf-fda0-4f5716102c6c"
      },
      "source": [
        " [token for token in doc]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Você, encontrou, o, livro, que, eu, te, falei, ,, Carla, ?]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMuyhneHcLXp",
        "colab_type": "text"
      },
      "source": [
        "Repare que agora a estrutura considerou a pontução e as palavras como estruturas separadas. Também não temos mais uma lista de strings, mas uma lista de Tokens.\n",
        "\n",
        "Se não quisermos os objetos Tokens, mas sim as strings que cada Token contém podemos usar o método .orth_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQWcizB3cKWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e77e152f-105d-4913-c573-e41ab332897b"
      },
      "source": [
        "[token.orth_ for token in doc]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Você',\n",
              " 'encontrou',\n",
              " 'o',\n",
              " 'livro',\n",
              " 'que',\n",
              " 'eu',\n",
              " 'te',\n",
              " 'falei',\n",
              " ',',\n",
              " 'Carla',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2KIw1YGcRMV",
        "colab_type": "text"
      },
      "source": [
        "# Entendendo diferença entre palavras e pontuações\n",
        "Como o spaCy entende que existe uma diferença entre uma palavra e uma pontuação, também podemos fazer filtragens. E se eu quisesse apenas as palavras da frase?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbn_ayXKcN42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9da7edbc-267d-440b-e698-e1d58ade3206"
      },
      "source": [
        "[token.orth_ for token in doc if not token.is_punct]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'te', 'falei', 'Carla']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3fy-oxPcWsX",
        "colab_type": "text"
      },
      "source": [
        "# Similaridade\n",
        "O spaCy também permite avaliar similaridade entre palavras. O método .similarity de um Token avalia a similaridade semântica estimada entre as palavras. Quanto maior o valor, mais similar são as palavras.\n",
        "\n",
        "Vamos avaliar a similaridade entre 3 palavras: você, livro e eu.\n",
        "\n",
        "Primeiro vamos armazenar o tokens em uma lista para acessá-los de forma independente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNha45C8cUg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = [token for token in doc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQunzw1Pcd2M",
        "colab_type": "text"
      },
      "source": [
        "Dessa forma, temos que tokens[0] representa o meu token da palavra Você e tokens[5] representa a palavra eu. A análise de similaridade pode ser dada por:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfddlqhGcZwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1e0f5c59-738e-49ac-8938-09ee3175bfb0"
      },
      "source": [
        "tokens[0].similarity(tokens[5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4892635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTf94WvIcjVc",
        "colab_type": "text"
      },
      "source": [
        "Legal, temos um valor de 0,29. E o que isso significa? Sabemos intuitivamente que eu e você devem ser muito mais similares que você e livro, por exemplo. Vamos investigar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0vlJGsAcfhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c9460f36-96b9-4634-fed1-5ed9b8f284c0"
      },
      "source": [
        "tokens[0].similarity(tokens[3])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0045953672"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYo7ppY0c1MX",
        "colab_type": "text"
      },
      "source": [
        "O valor é negativo, ou seja, você de fato está semânticamente muito mais próximo de eu do que de livro, o que faz todo sentido, certo?\n",
        "\n",
        "# Análise de classes gramaticais\n",
        "Podemos também entender as classes gramaticais de cada palavra dentro do nosso contexto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qri2kKoec8T3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "12ea6160-379f-4aab-8096-2729f5918195"
      },
      "source": [
        "[(token.orth_, token.pos_) for token in doc]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Você', 'PRON'),\n",
              " ('encontrou', 'VERB'),\n",
              " ('o', 'DET'),\n",
              " ('livro', 'NOUN'),\n",
              " ('que', 'PRON'),\n",
              " ('eu', 'PRON'),\n",
              " ('te', 'PRON'),\n",
              " ('falei', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('Carla', 'PROPN'),\n",
              " ('?', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4hIb1vrc_vr",
        "colab_type": "text"
      },
      "source": [
        "Então assim conseguimos ver que encontrou e falei são os verbos da frase. E a vírgula e o ponto de interrogação foi corretamente definido como pontuação (PUNCT).\n",
        "\n",
        "# Encontrei, encontraram, encontrarão, encontrariam….\n",
        "Agora, imagine que você tem um texto enorme e diversos tempos verbais diferentes. A análise passa a ser infinitamente mais complicada! O que podemos fazer, então, é analisar não o verbo no tempo verbal que ele foi escrito, mas ele em sua raiz. O nome desse método de encontrar a raiz das palavras é lematização por isso, o método .lemma_ faz exatamente isso. Vamos olhar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2mnLyiBdCbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef36dfd7-5dd3-48d0-983a-53760a287a59"
      },
      "source": [
        "[token.lemma_ for token in doc if token.pos_ == 'VERB']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['encontrar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeKDIqa8dGXY",
        "colab_type": "text"
      },
      "source": [
        "E isso vale para diversos tempos verbais MESMO!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEh1ufQZdJYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c2c6881-387a-49e9-98de-746eb554d364"
      },
      "source": [
        "doc = nlp(u'encontrei, encontraram, encontrarão, encontrariam')\n",
        "[token.lemma_ for token in doc if token.pos_ == 'VERB']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['encontrar', 'encontrar', 'encontrar', 'encontrar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjNpiXkidM0M",
        "colab_type": "text"
      },
      "source": [
        "# Ancestrais\n",
        "Do mesmo jeito que podemos encontrar as raízes de uma palavra, podemos checar se uma palavra é raíz de o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8_ZwrurdPd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e496bf21-f445-4c94-d5d5-c378219b26c9"
      },
      "source": [
        "doc = nlp(u'encontrar encontrei')\n",
        "tokens = [token for token in doc]\n",
        "tokens[0].is_ancestor(tokens[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgmS-PXjdVhl",
        "colab_type": "text"
      },
      "source": [
        "# E por fim… entidades\n",
        "Por fim, podemos avaliar as entidades presentes em uma frase. Por exemplo, peguemos a frase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8xx11indYE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(u'Machado de Assis um dos melhores escritores do Brasil, foi o primeiro presidente da Academia Brasileira de Letras')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfji8g6RdakT",
        "colab_type": "text"
      },
      "source": [
        "Se formos analisar as entidades presentes nessa frase, percebemos que temos 3 entidades identificadas automaticamente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qacExKjiddG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8380765f-c0e9-44e8-a137-18d8a5ce4247"
      },
      "source": [
        "doc.ents"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Machado de Assis, Brasil, Academia Brasileira de Letras)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evIjtA0LdfO4",
        "colab_type": "text"
      },
      "source": [
        "Ao analisarmos detalhadamente, podemos ver que o Spacy identificou Machado de Assis como uma pessoa (PER de person, em inglês), Brasil como um local (LOC) e Academia Brasileira de Letras como uma organização (ORG)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa7xsjyadhsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "66c05e97-0fa7-4089-988d-85551d7450ba"
      },
      "source": [
        "[(entity, entity.label_) for entity in doc.ents]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Machado de Assis, 'PER'),\n",
              " (Brasil, 'LOC'),\n",
              " (Academia Brasileira de Letras, 'ORG')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gciGSAtydj_k",
        "colab_type": "text"
      },
      "source": [
        "Claro que em inglês o modelo está bem avançado, e consegue identificar entidades bem mais complexas do que eu pude verificar nos textos em português. Por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ecFT1McdoVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3f8291f5-2f33-4b73-a858-c70bc18fac59"
      },
      "source": [
        "wiki_obama = \"\"\"Barack Obama is an American politician who served as\n",
        "     the 44th President of the United States from 2009 to 2017. He is the first\n",
        "     African American to have served as president,\n",
        "     as well as the first born outside the contiguous United States.\"\"\"\n",
        "nlp = spacy.load('en')\n",
        "nlp_obama = nlp(wiki_obama)\n",
        "[(i, i.label_) for i in nlp_obama.ents]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Barack Obama, 'PERSON'),\n",
              " (American, 'NORP'),\n",
              " (44th, 'ORDINAL'),\n",
              " (the United States, 'GPE'),\n",
              " (2009, 'DATE'),\n",
              " (2017, 'DATE'),\n",
              " (first, 'ORDINAL'),\n",
              " (African American, 'NORP'),\n",
              " (first, 'ORDINAL'),\n",
              " (United States, 'GPE')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}